{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predict Tomorrow's Closing Price of a Stock\nThis notebook was built to satisfy a homework assignment for the Deep Learning class at the University of Colorado, Boulder. Below I will attempt to predict the following day's close for a stock. Stock trading is a highly valuable exercise, so if we can develop a model would be priceless. I will use LSTM and GRU recurrent models to make two attempts at getting a working model. I will also go through several optimizers to see which performs best on each of the classes of units (LSTM and GRU).\n\n#### The data:\nStock market data is both well understood, and readily available. Each stock has a closing price for each day of trading, the data is regimented, and the SEC watches over the system for irregularities. We will get our data from the Yahoo Finance module so that we have a reputable source. There isn't much exploratory data analysis to do, but we will characterize and inspect our data before modeling.\n\nCitation:\n* Aroussi, R. Yfinance 0.2.36 Documentation. PyPI YFinance. https://pypi.org/project/yfinance/ \n\n\n#### Work to do:\n1. Load Libraries.\n2. Build Function to get stock data.\n3. EDA on a few random stocks.\n4. Build the LSTM Model\n5. Train and Test Model on 3 Optimizers\n6. Build the GRU Model\n7. Train and Test Model on 3 Optimizers\n8. Collate Results and Visualize\n9. Conclusion:\n\n#### CAVEAT: \nBecause yfinance scrapes data for a few of it’s functions, you sometimes run the risk of getting rate limited or blacklisted for too many scraping attempts. This is a risk that’s always present when trying to scrape websites, but when you’re building applications trading real money on top of infrastructure that might be making a lot of data requests, the risk/reward calculus changes. *Do not use this script to trade real money*.\n\n# Load Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n!pip install yfinance\nimport yfinance  as yf\nfrom sklearn.preprocessing import RobustScaler\nfrom collections import deque\n \nimport tensorflow as tf\nprint(\"TF Version:\", tf.__version__)\nfrom keras.models import Sequential\nfrom keras.layers import Dense, GRU, LSTM, Dropout, Input\nimport os, warnings, matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-08T04:46:02.329916Z","iopub.execute_input":"2024-02-08T04:46:02.330279Z","iopub.status.idle":"2024-02-08T04:46:52.623572Z","shell.execute_reply.started":"2024-02-08T04:46:02.330252Z","shell.execute_reply":"2024-02-08T04:46:52.622370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### CAVEAT: \nBecause yfinance scrapes data for a few of it’s functions, you sometimes run the risk of getting rate limited or blacklisted for too many scraping attempts. This is a risk that’s always present when trying to scrape websites, but when you’re building applications trading real money on-top of infrastructure that might be making a lot of data requests, the risk:reward changes. Do not use this script to trade real money.\n\n\n\n# Build Function to Gather Stock Data","metadata":{}},{"cell_type":"code","source":"# Function to extract stock data from yfinance\ndef Extract_Data(Stock = 'BA',   intervalmos = 12, interval = '1d'):\n    STOCK = Stock\n    now = datetime.today() # Current date  \n    DATE_NOW = now.strftime('%Y-%m-%d') # Extract today's date as a string \n    #set the interval start date string\n    if int(intervalmos) > 0:\n        DATE_THEN = now - relativedelta(months=intervalmos)\n        DATE_THEN = DATE_THEN.strftime('%Y-%m-%d')\n    else:\n        DATE_THEN = now - relativedelta(months=12)\n        DATE_THEN = DATE_THEN.strftime('%Y-%m-%d')\n    #load the data into a dataframe and return it\n    raw_price_df = yf.download(STOCK, start=DATE_THEN, end=DATE_NOW, interval=interval)\n    return raw_price_df\n\n# test the function with Boeing for 12 mos, at 1hr interval\n# available intervals: 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo\n# important to note that the 1m data is only retrievable for the last 7 days, and anything intraday (interval <1d) only for the last 60 days\ntestdata = Extract_Data('BA',12,'1d')\ntestdata","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:46:52.625682Z","iopub.execute_input":"2024-02-08T04:46:52.626500Z","iopub.status.idle":"2024-02-08T04:47:09.996621Z","shell.execute_reply.started":"2024-02-08T04:46:52.626459Z","shell.execute_reply":"2024-02-08T04:47:09.995517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great. We have stock data for a stock, for each trading day in the last year (251 days = 50 weeks * 5 days +1). \n\n# Cursory Exploratory Data Analysis\n\nLet's see if there are any NaNs, empty cells, and look to see if the data is normally distributed for all price metrics. The volume metric should have a gamma or exponential distribution.","metadata":{}},{"cell_type":"code","source":"# Test for empty cells or NaN\nprint(\"Any NaNs?\")\nprint(testdata.isna().sum())\nprint('\\nAny Empty Cells?')\nprint(testdata.info())","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:09.997807Z","iopub.execute_input":"2024-02-08T04:47:09.998156Z","iopub.status.idle":"2024-02-08T04:47:10.019049Z","shell.execute_reply.started":"2024-02-08T04:47:09.998129Z","shell.execute_reply":"2024-02-08T04:47:10.017699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(2,3,figsize=(12,6))\nj=0\nh=0\nfor i in range(6):\n    ax[j,h].hist(testdata.iloc[:,i])\n    ax[j,h].set_xlabel(testdata.columns[i])\n  \n    if j == 1:\n        h += 1\n        j = 0\n    else:\n        j +=1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:10.021477Z","iopub.execute_input":"2024-02-08T04:47:10.021915Z","iopub.status.idle":"2024-02-08T04:47:10.825312Z","shell.execute_reply.started":"2024-02-08T04:47:10.021882Z","shell.execute_reply":"2024-02-08T04:47:10.824221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, the data is clean and regimented. All the price metrics have a normal distribution. The volume metric has a gamma or exponential distribution. We can change the testdata function to test any stock we like. Moving on...\n\nBefore we build the model, let's build a better visualizer and a function to preprocess data for the ensuing model.","metadata":{}},{"cell_type":"code","source":"# function to visualize the extracted closing price data\ndef Visualize_Data(df, Stock,  figsize = (20,9)):\n    #plt.style.use(style='ggplot')\n    plt.figure(figsize=figsize)\n    plt.plot(df['Close'],color='b')\n    plt.xlabel('Date')\n    plt.ylabel('Price in $')\n    plt.legend([f'Price of {Stock} Share'])\n    plt.title(f'Share Prices for {Stock} Stock')\n    plt.show()\n\n#test the visualizer\nVisualize_Data(testdata, 'BA')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:10.826536Z","iopub.execute_input":"2024-02-08T04:47:10.826944Z","iopub.status.idle":"2024-02-08T04:47:11.180661Z","shell.execute_reply.started":"2024-02-08T04:47:10.826916Z","shell.execute_reply":"2024-02-08T04:47:11.179729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need another function to prepare the data for the models below so we only write this once. We'll also make the standard scaler a global function to se can use it inside the function and outside later.","metadata":{}},{"cell_type":"code","source":"#set scaler as a global variable to access inside of the function and outside later...\nglobal scaler\nscaler = RobustScaler()\n\n# Function to process data set for training\ndef Prepare_Data(dataframe, days, stepper):\n    # copy the dataframe before we play with it\n    df = dataframe.copy()\n    \n    # Cleave off the number of days to predict from bottom of dataframe\n    df = df.iloc[:(df.shape[0]-stepper+1),:]\n    # print(df.shape)\n    \n    # Pre-process the DataFrame\n    df = df.drop(['Open', 'High', 'Low', 'Adj Close',  'Volume'], axis=1)\n    df['date'] = df.index\n    \n    # use global scaler function to fit between 0 and 1\n    df['scaled_close'] = scaler.fit_transform(np.expand_dims(df['Close'].values, axis=1))\n    #print(df.scaled_close)\n    \n    # Engineer the DataFrame to add future and start defining the last sequence\n    df['future'] = df['scaled_close'].shift(-(days))\n    #print(len(df.future))\n    last_sequence = np.array(df[['scaled_close']].tail(days))\n    #print(last_sequence)\n    \n    # Just in case, drop and NaNs\n    df.dropna(inplace=True)  \n    \n    # Define Variables for sequencing\n    sequence_data = []\n    sequences = deque(maxlen=days)\n    #print(len(df.future))\n    \n    # Generate arrays of close data and dates up to the last sequence...\n    for entry, target in zip(df[['scaled_close','date']].values, df['future'].values):\n        sequences.append(entry)\n        if len(sequences) == days:\n            sequence_data.append([np.array(sequences), target])\n    #print(sequence_data)\n    \n    # Modify last sequence to remove close info and flip to array\n    last_sequence = list([x[:1] for x in sequences]) + list(last_sequence)\n    last_sequence = np.array(last_sequence).astype(np.float32)\n\n    # build X and Y training set\n    X, Y = [], []\n    for seq, target in sequence_data:\n        X.append(seq)\n        Y.append(target)\n\n    # convert X and Y to numpy arrays for compatibility\n    X = np.array(X)\n    Y = np.array(Y)\n\n    return last_sequence, X, Y\n\n# Test the preprocessor and view shapes to ensure they add up...\na, b = 12,1\nte = 252 - a -  1 + b\nprint(\"Test days:\",te)\ntested = Prepare_Data(testdata,a,b)\nprint(\"last_sequence\",tested[0].shape,\"\\n X:\",tested[1].shape,\"\\n Y:\",tested[2].shape, \"\\nTotal Days Match:\", tested[0].shape[0]+tested[1].shape[0]-a == te )\n#tested[1]","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:11.182308Z","iopub.execute_input":"2024-02-08T04:47:11.182646Z","iopub.status.idle":"2024-02-08T04:47:11.206549Z","shell.execute_reply.started":"2024-02-08T04:47:11.182616Z","shell.execute_reply":"2024-02-08T04:47:11.205739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great. With data at the ready, we can move on to building the Models, Training, etc.\n\n# Build LSTM Model\n\nTo begin, we'll define a function to build a simple Sequential model with 2 layers of LSTM and Dropout that returns a single value. We'll build the function with loss, optimizer, epoch, and batch size inputs to iterate through several options and record results easily.","metadata":{}},{"cell_type":"code","source":"\ndef Train_Model(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS):\n\n    model = Sequential([\n        Input(shape=(NUMBER_of_STEPS_BACK,1),batch_size= BATCH_SIZE),\n        LSTM(UNITS, return_sequences=True), # use_bias=True,stateful=True, recurrent_initializer='orthogonal'),\n        Dropout(DROPOUT),\n        LSTM(UNITS, return_sequences=False),\n        Dropout(DROPOUT),\n        Dense(1)\n        ])\n    #model.summary()\n    model.compile(loss=LOSS, optimizer=OPTIMIZER)\n    model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:11.207892Z","iopub.execute_input":"2024-02-08T04:47:11.208438Z","iopub.status.idle":"2024-02-08T04:47:11.213874Z","shell.execute_reply.started":"2024-02-08T04:47:11.208410Z","shell.execute_reply":"2024-02-08T04:47:11.213161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Stock to model, and YFinance pull characteristics\nSTOCK = 'BA' # Specify the ticker symbol \nLENGTH = 12 # Length of the extracted data in months\nINTERVAL = '1d' # Interval (see options above)\nPREDICTION_STEPS = 5 # Number of int(days) the model(s) will predict. To predict the next three days change to 3.\n\n#Set Variable Parameters/Hyperparameters\nif INTERVAL == '4h':\n    NUMBER_of_STEPS_BACK = 30 * 2 # Number of days * 3 times per day that the model will be trained \nelif INTERVAL == '1h':\n    NUMBER_of_STEPS_BACK = 30 * 2 # Number of days * 7 hrs per day that the model will be trained \nelse: # INTERVAL == '1d'\n    NUMBER_of_STEPS_BACK = 17  # Number of days back that the model will be trained   \n\n# Visualize Data\nraw_price_df = Extract_Data(Stock = STOCK, intervalmos = LENGTH,interval =    INTERVAL )\nVisualize_Data(raw_price_df, Stock = STOCK, )\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:11.215227Z","iopub.execute_input":"2024-02-08T04:47:11.215499Z","iopub.status.idle":"2024-02-08T04:47:11.599188Z","shell.execute_reply.started":"2024-02-08T04:47:11.215477Z","shell.execute_reply":"2024-02-08T04:47:11.598109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have stock data to model, a modeling function, and can see it, we define parameters for the first model and see what we get.","metadata":{}},{"cell_type":"code","source":"#Set Constant Parameters/Hyperparameters\nBATCH_SIZE = 25 # Number of training samples that will be passed per step\nDROPOUT = 0.25 # To improve performance by regularization  \nUNITS = 60 # Number of LSTM neurons in each layer\nEPOCHS = 10 # Number of iterations for model training\nLOSS='mean_squared_error' # Loss Methodology \nOPTIMIZER='adam' # Optimizer \n\n# Make Prediction\npredictions = []\n\nfor step in range(PREDICTION_STEPS):\n    stepped = PREDICTION_STEPS - step\n    last_sequence, x_train, y_train = Prepare_Data(raw_price_df, NUMBER_of_STEPS_BACK, stepped)\n    #print(\"Sequence Pull Shape:\",last_sequence.shape)\n    x_train = x_train[:, :, :1].astype(np.float32)\n\n    model = Train_Model(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS)\n\n    last_sequence = last_sequence[-(NUMBER_of_STEPS_BACK):]\n    last_sequence = np.expand_dims(last_sequence, axis=0)\n    prediction = model.predict(last_sequence)\n    print(prediction)\n    predicted_price = scaler.inverse_transform(prediction)[0][0]\n  \n    predictions.append(round(float(predicted_price), 2))\n    \n# Print Predictions\nif len(predictions) > 0:\n    predictions_list = ['$'+str(d) for d in predictions]\n    predictions_str = ', '.join(predictions_list)\n    message = f'{STOCK} share price prediction(s) for next {len(predictions)} day(s) {predictions_str}'\n    print(message)\n\nLSTMdf = pd.DataFrame({\"Date\":list(raw_price_df.index)[-PREDICTION_STEPS:],\n                       \"Actual Close\":[round(x,2) for x in list(raw_price_df.Close.tail(PREDICTION_STEPS).values)],\n                       \"Predicted Close\":predictions \n                       })\nLSTMdf['Difference'] = [round(x,2) for x in np.array(LSTMdf['Actual Close']).astype(float)-predictions]\nLSTMdf[\"Model\"] = 'LSTM'\nLSTMdf[\"Optimizer\"] = \"Adam\"\nLSTMdf[\"Batch Size\"] = BATCH_SIZE\n\nLSTMdf","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:11.600484Z","iopub.execute_input":"2024-02-08T04:47:11.600808Z","iopub.status.idle":"2024-02-08T04:47:43.940707Z","shell.execute_reply.started":"2024-02-08T04:47:11.600782Z","shell.execute_reply":"2024-02-08T04:47:43.939675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Set Constant Parameters/Hyperparameters\nBATCH_SIZE = 15 # Number of training samples that will be passed per step\nDROPOUT = 0.25 # To improve performance by regularization  \nUNITS = 60 # Number of LSTM neurons in each layer\nEPOCHS = 10 # Number of iterations for model training\nLOSS='mean_squared_error' # Loss Methodology \nOPTIMIZER='RMSprop' # Optimizer \n\n# Make Prediction\npredictions = []\n\nfor step in range(PREDICTION_STEPS):\n    stepped = PREDICTION_STEPS - step\n    last_sequence, x_train, y_train = Prepare_Data(raw_price_df, NUMBER_of_STEPS_BACK, stepped)\n    #print(\"Sequence Pull Shape:\",last_sequence.shape)\n    x_train = x_train[:, :, :1].astype(np.float32)\n\n    model = Train_Model(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS)\n\n    last_sequence = last_sequence[-(NUMBER_of_STEPS_BACK):]\n    last_sequence = np.expand_dims(last_sequence, axis=0)\n    prediction = model.predict(last_sequence)\n    print(prediction)\n    predicted_price = scaler.inverse_transform(prediction)[0][0]\n  \n    predictions.append(round(float(predicted_price), 2))\n    \n# Print Predictions\nif len(predictions) > 0:\n    predictions_list = ['$'+str(d) for d in predictions]\n    predictions_str = ', '.join(predictions_list)\n    message = f'{STOCK} share price prediction(s) for next {len(predictions)} day(s) {predictions_str}'\n    print(message)\n\nLSTM2df = pd.DataFrame({\"Date\":list(raw_price_df.index)[-PREDICTION_STEPS:],\n                       \"Actual Close\":[round(x,2) for x in list(raw_price_df.Close.tail(PREDICTION_STEPS).values)],\n                       \"Predicted Close\":predictions\n                       })\nLSTM2df['Difference'] = [round(x,2) for x in np.array(LSTMdf['Actual Close']).astype(float)-predictions]\nLSTM2df[\"Model\"] = 'LSTM'\nLSTM2df[\"Optimizer\"] = OPTIMIZER\nLSTM2df[\"Batch Size\"] = BATCH_SIZE\n\nLSTM2df","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:47:43.945044Z","iopub.execute_input":"2024-02-08T04:47:43.945357Z","iopub.status.idle":"2024-02-08T04:48:18.465475Z","shell.execute_reply.started":"2024-02-08T04:47:43.945331Z","shell.execute_reply":"2024-02-08T04:48:18.464380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Constant Parameters/Hyperparameters\nBATCH_SIZE = 12 # Number of training samples that will be passed per step\nDROPOUT = 0.25 # To improve performance by regularization  \nUNITS = 60 # Number of LSTM neurons in each layer\nEPOCHS = 10 # Number of iterations for model training\nLOSS='mean_squared_error' # Loss Methodology \nOPTIMIZER='SGD' # Optimizer \n\n# Make Prediction\npredictions = []\n\nfor step in range(PREDICTION_STEPS):\n    stepped = PREDICTION_STEPS - step\n    last_sequence, x_train, y_train = Prepare_Data(raw_price_df, NUMBER_of_STEPS_BACK, stepped)\n    #print(\"Sequence Pull Shape:\",last_sequence.shape)\n    x_train = x_train[:, :, :1].astype(np.float32)\n\n    model = Train_Model(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS)\n\n    last_sequence = last_sequence[-(NUMBER_of_STEPS_BACK):]\n    last_sequence = np.expand_dims(last_sequence, axis=0)\n    prediction = model.predict(last_sequence)\n    print(prediction)\n    predicted_price = scaler.inverse_transform(prediction)[0][0]\n  \n    predictions.append(round(float(predicted_price), 2))\n    \n# Print Predictions\nif len(predictions) > 0:\n    predictions_list = ['$'+str(d) for d in predictions]\n    predictions_str = ', '.join(predictions_list)\n    message = f'{STOCK} share price prediction(s) for next {len(predictions)} day(s) {predictions_str}'\n    print(message)\n\nLSTM3df = pd.DataFrame({\"Date\":list(raw_price_df.index)[-PREDICTION_STEPS:],\n                       \"Actual Close\":[round(x,2) for x in list(raw_price_df.Close.tail(PREDICTION_STEPS).values)],\n                       \"Predicted Close\":predictions\n                       })\nLSTM3df['Difference'] = [round(x,2) for x in np.array(LSTMdf['Actual Close']).astype(float)-predictions]\nLSTM3df[\"Model\"] = 'LSTM'\nLSTM3df[\"Optimizer\"] = OPTIMIZER\nLSTM3df[\"Batch Size\"] = BATCH_SIZE\n\nLSTM3df","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:48:18.466886Z","iopub.execute_input":"2024-02-08T04:48:18.467291Z","iopub.status.idle":"2024-02-08T04:48:53.076700Z","shell.execute_reply.started":"2024-02-08T04:48:18.467256Z","shell.execute_reply":"2024-02-08T04:48:53.075611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, now that we have 2 models worth of results let's see how they compare. We'll compute average difference between the predicted close and the actual close and concatenate the two dataframes to get a clear view into the data.","metadata":{}},{"cell_type":"code","source":"LSTMav = round(np.mean(LSTMdf.Difference),3)\nLSTM2av = round(np.mean(LSTM2df.Difference),3)\nLSTM3av = round(np.mean(LSTM3df.Difference),3)\nprint(f\"Averages: LSTM with Adam Optimizer: {LSTMav}  -  LSTM with RMSprop Optimizer: {LSTM2av}  - LSTM with SGD Optimizer: {LSTM3av}\")\nLSTout = pd.concat([LSTMdf,LSTM2df,LSTM3df]) \nLSTout","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:54:09.139745Z","iopub.execute_input":"2024-02-08T04:54:09.140139Z","iopub.status.idle":"2024-02-08T04:54:09.160912Z","shell.execute_reply.started":"2024-02-08T04:54:09.140113Z","shell.execute_reply":"2024-02-08T04:54:09.159799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RMSprop (Root Mean Square Propogation) optimizer beat the Adam optimizer and SGD (Stochastic Gradient Descent) out of the box (we didn't specifiy a number of parameters including learning rate for any of them). We should notice that we gave the RMSprop a worse chance by giving it fewer observations in each of the batches of data to use in modeling. Were the difference wholly attributable to batch size, SGD would have won as it has the smallest batch size. We can run a bunch more tests here to improve these models, but we'll move on because this is a showcase of LSTM capabilities not a competition.\n\n# Build GRU Model\n\nThe same process will be repeated for GRU units in the model instead of LSTM. Again, we'll build a function to build, compile, and train the models from a series of variable inputs. This will help us iterate through several options with a minimum amount of code.","metadata":{}},{"cell_type":"code","source":"def Train_ModelG(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS):\n\n    model = Sequential([\n        Input(shape=(NUMBER_of_STEPS_BACK,1),batch_size= BATCH_SIZE),\n        GRU(UNITS, return_sequences=True), # use_bias=True,stateful=True, recurrent_initializer='orthogonal'),\n        Dropout(DROPOUT),\n        GRU(UNITS, return_sequences=False),\n        Dropout(DROPOUT),\n        Dense(1)\n        ])\n    #model.summary()\n    model.compile(loss=LOSS, optimizer=OPTIMIZER)\n    model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:48:53.099984Z","iopub.execute_input":"2024-02-08T04:48:53.100326Z","iopub.status.idle":"2024-02-08T04:48:53.112735Z","shell.execute_reply.started":"2024-02-08T04:48:53.100300Z","shell.execute_reply":"2024-02-08T04:48:53.111647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Constant Parameters/Hyperparameters\nBATCH_SIZE = 25 # Number of training samples that will be passed per step\nDROPOUT = 0.25 # To improve performance by regularization  \nUNITS = 60 # Number of GRU neurons in each layer\nEPOCHS = 10 # Number of iterations for model training\nLOSS='mean_squared_error' # Loss Methodology \nOPTIMIZER='adam' # Optimizer \n\n# Make Prediction\npredictions = []\n\nfor step in range(PREDICTION_STEPS):\n    stepped = PREDICTION_STEPS - step\n    last_sequence, x_train, y_train = Prepare_Data(raw_price_df, NUMBER_of_STEPS_BACK, stepped)\n    #print(\"Sequence Pull Shape:\",last_sequence.shape)\n    x_train = x_train[:, :, :1].astype(np.float32)\n\n    modelg = Train_ModelG(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS)\n\n    last_sequence = last_sequence[-(NUMBER_of_STEPS_BACK):]\n    last_sequence = np.expand_dims(last_sequence, axis=0)\n    prediction = modelg.predict(last_sequence)\n    print(prediction)\n    predicted_price = scaler.inverse_transform(prediction)[0][0]\n  \n    predictions.append(round(float(predicted_price), 2))\n    \n# Print Predictions\nif len(predictions) > 0:\n    predictions_list = ['$'+str(d) for d in predictions]\n    predictions_str = ', '.join(predictions_list)\n    message = f'{STOCK} share price prediction(s) for next {len(predictions)} day(s) {predictions_str}'\n    print(message)\n\nGRUdf = pd.DataFrame({\"Date\":list(raw_price_df.index)[-PREDICTION_STEPS:],\n                       \"Actual Close\":[round(x,2) for x in list(raw_price_df.Close.tail(PREDICTION_STEPS).values)],\n                       \"Predicted Close\":predictions\n                       })\nGRUdf['Difference'] = [round(x,2) for x in np.array(LSTMdf['Actual Close']).astype(float)-predictions]\nGRUdf[\"Model\"] = 'GRU'\nGRUdf[\"Optimizer\"] = OPTIMIZER\nGRUdf[\"Batch Size\"] = BATCH_SIZE\n\nGRUdf","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:48:53.114131Z","iopub.execute_input":"2024-02-08T04:48:53.114433Z","iopub.status.idle":"2024-02-08T04:49:25.532084Z","shell.execute_reply.started":"2024-02-08T04:48:53.114409Z","shell.execute_reply":"2024-02-08T04:49:25.530909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Constant Parameters/Hyperparameters\nBATCH_SIZE = 15 # Number of training samples that will be passed per step\nDROPOUT = 0.25 # To improve performance by regularization  \nUNITS = 60 # Number of LSTM neurons in each layer\nEPOCHS = 10 # Number of iterations for model training\nLOSS='mean_squared_error' # Loss Methodology \nOPTIMIZER='RMSprop' # Optimizer \n\n# Make Prediction\npredictions = []\n\nfor step in range(PREDICTION_STEPS):\n    stepped = PREDICTION_STEPS - step\n    last_sequence, x_train, y_train = Prepare_Data(raw_price_df, NUMBER_of_STEPS_BACK, stepped)\n    #print(\"Sequence Pull Shape:\",last_sequence.shape)\n    x_train = x_train[:, :, :1].astype(np.float32)\n\n    modelg = Train_ModelG(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS)\n\n    last_sequence = last_sequence[-(NUMBER_of_STEPS_BACK):]\n    last_sequence = np.expand_dims(last_sequence, axis=0)\n    prediction = modelg.predict(last_sequence)\n    print(prediction)\n    predicted_price = scaler.inverse_transform(prediction)[0][0]\n  \n    predictions.append(round(float(predicted_price), 2))\n    \n# Print Predictions\nif len(predictions) > 0:\n    predictions_list = ['$'+str(d) for d in predictions]\n    predictions_str = ', '.join(predictions_list)\n    message = f'{STOCK} share price prediction(s) for next {len(predictions)} day(s) {predictions_str}'\n    print(message)\n\nGRU2df = pd.DataFrame({\"Date\":list(raw_price_df.index)[-PREDICTION_STEPS:],\n                       \"Actual Close\":[round(x,2) for x in list(raw_price_df.Close.tail(PREDICTION_STEPS).values)],\n                       \"Predicted Close\":predictions\n                       })\nGRU2df['Difference'] = [round(x,2) for x in np.array(LSTMdf['Actual Close']).astype(float)-predictions]\nGRU2df[\"Model\"] = 'GRU'\nGRU2df[\"Optimizer\"] = OPTIMIZER\nGRU2df[\"Batch Size\"] = BATCH_SIZE\n\nGRU2df","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:49:25.533349Z","iopub.execute_input":"2024-02-08T04:49:25.533829Z","iopub.status.idle":"2024-02-08T04:50:00.167803Z","shell.execute_reply.started":"2024-02-08T04:49:25.533802Z","shell.execute_reply":"2024-02-08T04:50:00.166674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set Constant Parameters/Hyperparameters\nBATCH_SIZE = 12 # Number of training samples that will be passed per step\nDROPOUT = 0.25 # To improve performance by regularization  \nUNITS = 60 # Number of LSTM neurons in each layer\nEPOCHS = 10 # Number of iterations for model training\nLOSS='mean_squared_error' # Loss Methodology \nOPTIMIZER='SGD' # Optimizer \n\n# Make Prediction\npredictions = []\n\nfor step in range(PREDICTION_STEPS):\n    stepped = PREDICTION_STEPS - step\n    last_sequence, x_train, y_train = Prepare_Data(raw_price_df, NUMBER_of_STEPS_BACK, stepped)\n    #print(\"Sequence Pull Shape:\",last_sequence.shape)\n    x_train = x_train[:, :, :1].astype(np.float32)\n\n    modelg = Train_ModelG(x_train, y_train, NUMBER_of_STEPS_BACK, BATCH_SIZE, UNITS, EPOCHS, DROPOUT, OPTIMIZER, LOSS)\n\n    last_sequence = last_sequence[-(NUMBER_of_STEPS_BACK):]\n    last_sequence = np.expand_dims(last_sequence, axis=0)\n    prediction = modelg.predict(last_sequence)\n    print(prediction)\n    predicted_price = scaler.inverse_transform(prediction)[0][0]\n  \n    predictions.append(round(float(predicted_price), 2))\n    \n# Print Predictions\nif len(predictions) > 0:\n    predictions_list = ['$'+str(d) for d in predictions]\n    predictions_str = ', '.join(predictions_list)\n    message = f'{STOCK} share price prediction(s) for next {len(predictions)} day(s) {predictions_str}'\n    print(message)\n\nGRU3df = pd.DataFrame({\"Date\":list(raw_price_df.index)[-PREDICTION_STEPS:],\n                       \"Actual Close\":[round(x,2) for x in list(raw_price_df.Close.tail(PREDICTION_STEPS).values)],\n                       \"Predicted Close\":predictions\n                       })\nGRU3df['Difference'] = [round(x,2) for x in np.array(LSTMdf['Actual Close']).astype(float)-predictions]\nGRU3df[\"Model\"] = 'GRU'\nGRU3df[\"Optimizer\"] = OPTIMIZER\nGRU3df[\"Batch Size\"] = BATCH_SIZE\n\nGRU3df","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:50:00.169341Z","iopub.execute_input":"2024-02-08T04:50:00.169777Z","iopub.status.idle":"2024-02-08T04:50:34.384681Z","shell.execute_reply.started":"2024-02-08T04:50:00.169739Z","shell.execute_reply":"2024-02-08T04:50:34.383738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GRUav = round(np.mean(GRUdf.Difference),4)\nGRU2av = round(np.mean(GRU2df.Difference),4)\nGRU3av = round(np.mean(GRU3df.Difference),4)\nprint(f\"Averages: GRU with Adam Optimizer: {GRUav}  -  GRU with RMSprop Optimizer: {GRU2av}  - GRU with SGD Optimizer: {GRU3av}\")\nGRUout = pd.concat([GRUdf,GRU2df,GRU3df]) \nGRUout","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:53:30.485738Z","iopub.execute_input":"2024-02-08T04:53:30.486178Z","iopub.status.idle":"2024-02-08T04:53:30.507202Z","shell.execute_reply.started":"2024-02-08T04:53:30.486128Z","shell.execute_reply":"2024-02-08T04:53:30.506029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When using the GRU units, the SGD (Stochastic Gradient Descent) optimizer beats out the RMSProp and Adam optimizers. This is a bit different than the LSTM results. The same pattern of fewer observations leading to better model outcomes seems to hold here too. Here, too, we could spend much more time optimizing these models. Tuning hyperparameters, tuning optimzers, and batch sizes could well lead to much more prediction precision.\n\n# Results and Visualization\n\nNow that we have data from 2 models, and tree optimers for each, let's plot that data against actual closing price for the stock for the number of days.","metadata":{}},{"cell_type":"code","source":"allout = pd.concat([LSTout,GRUout])\nallout","metadata":{"execution":{"iopub.status.busy":"2024-02-08T04:54:27.042229Z","iopub.execute_input":"2024-02-08T04:54:27.043380Z","iopub.status.idle":"2024-02-08T04:54:27.064966Z","shell.execute_reply.started":"2024-02-08T04:54:27.043344Z","shell.execute_reply":"2024-02-08T04:54:27.063719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(raw_price_df.index.tolist()[-PREDICTION_STEPS:],\n         raw_price_df.Close.tail(PREDICTION_STEPS),\n         lw=5,\n         color='black')\nplt.plot(LSTMdf.Date,LSTMdf['Predicted Close'])\nplt.plot(LSTM2df.Date,LSTM2df['Predicted Close'])\nplt.plot(LSTM3df.Date,LSTM3df['Predicted Close'])\nplt.plot(GRUdf.Date,GRUdf['Predicted Close'])\nplt.plot(GRU2df.Date,GRU2df['Predicted Close'])\nplt.plot(GRU3df.Date,GRU3df['Predicted Close'])\nplt.xlabel(\"Date\")\nplt.ylabel('Price')\nplt.xticks(rotation=\"vertical\")\nplt.legend(['Actual','LSTM (Adam)','LSTM (RMSprop)','LSTM (SGD)','GRU (Adam)', 'GRU (RMSprop)', 'GRU (SGD)'], \n           fontsize='x-small')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-08T05:01:44.363033Z","iopub.execute_input":"2024-02-08T05:01:44.364147Z","iopub.status.idle":"2024-02-08T05:01:44.668434Z","shell.execute_reply.started":"2024-02-08T05:01:44.364114Z","shell.execute_reply":"2024-02-08T05:01:44.667381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThis notebook shows that a large bit of the variation in a stock's price can indeed be modeled by recurrent neural networks with LSTM or GRU units. Out of the box (untuned) the RMSprop optimizer seems to work well on these data with SGD coming ina close second. Important to notice that the predictions from these two optimizers don't typically agree on direction though. Some other interesting tidbits include the visual confirmation of the affect optimizers play in these models. The variation in the models isn't as uniform as the variation in the optimizers. For instance, the green and brown lines in the plot above mimic each other and are both SGD optimizer (one using LSTM model and one using GRU). The same can be gleaned from the orange and purple lines which are both RMSprop optimizer.\n\nCaveat to results: there wasn't an earnings release or other news this week that would have adversely affected these models. Stock patterns are only a portion of the underlying stock pricing mechanisms. Please don't use this model to automate trading as the yfinance package is unsupported.\n\nThis notebook is in no way complete. If I had more time I'd choose to tune the hyperparameters of the models and optimizers. Doing so could lead to increased accuracy. Other things that deserve attention in future iterations include tuning the batch sizes, playing with the interval (1min, 1hour, 1day) market data sequences, and the models themselves (varying the number of layers and units).\n\nIf you like this notebook or learned something an upvote is appreciated.","metadata":{}},{"cell_type":"code","source":"# Inspiration for this exploration came from this article:\n# https://medium.com/@onersarpnalcin/predicting-the-stock-market-with-lstm-via-tensorflow-in-python-bullish-or-bearish-tomorrow-3c4f2fc03c51\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-08T05:18:51.386065Z","iopub.execute_input":"2024-02-08T05:18:51.386513Z","iopub.status.idle":"2024-02-08T05:18:51.390692Z","shell.execute_reply.started":"2024-02-08T05:18:51.386481Z","shell.execute_reply":"2024-02-08T05:18:51.389919Z"},"trusted":true},"execution_count":null,"outputs":[]}]}